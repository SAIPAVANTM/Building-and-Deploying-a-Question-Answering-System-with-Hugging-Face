{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "def load_documents():\n",
        "    loader = PyPDFDirectoryLoader(\"/content/drive/MyDrive/GenAI\")\n",
        "    return loader.load()\n",
        "\n",
        "\n",
        "def split_documents(docs):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "    return text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "def create_embeddings():\n",
        "    return SentenceTransformerEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
        "\n",
        "\n",
        "def create_vectorstore(chunks, embeddings):\n",
        "    return Chroma.from_documents(chunks, embeddings)\n",
        "\n",
        "\n",
        "def initialize_llm():\n",
        "    return LlamaCpp(\n",
        "        model_path=\"/content/drive/MyDrive/BioMistral-7B.Q4_K_M.gguf\",\n",
        "        temperature=0.2,\n",
        "        max_tokens=2048,\n",
        "        top_p=1\n",
        "    )\n",
        "\n",
        "\n",
        "def create_prompt_template():\n",
        "    template = \"\"\"\n",
        "<|context|>\n",
        "You are a Medical Assistant that follows the instructions and generates accurate responses based on the query and the context provided.\n",
        "Please be truthful and give direct answers.\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    return ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "def create_rag_chain(retriever, llm, prompt):\n",
        "    return (\n",
        "        {\"context\": retriever, \"query\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set environment variables\n",
        "    os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n",
        "\n",
        "    # Load and process documents\n",
        "    docs = load_documents()\n",
        "    chunks = split_documents(docs)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = create_embeddings()\n",
        "    vectorstore = create_vectorstore(chunks, embeddings)\n",
        "\n",
        "    # Create retriever and LLM\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
        "    llm = initialize_llm()\n",
        "\n",
        "    # Create prompt template and RAG chain\n",
        "    prompt = create_prompt_template()\n",
        "    rag_chain = create_rag_chain(retriever, llm, prompt)\n",
        "\n",
        "    # Interactive loop\n",
        "    while True:\n",
        "        user_input = input(\"Input query: \")\n",
        "        if user_input == \"exit\":\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        if user_input == \"\":\n",
        "            continue\n",
        "\n",
        "        result = rag_chain.invoke(user_input)\n",
        "        f1 = rag_chain.invoke(\"whats the f1 score of the previous query, just give score dont need explanation\")\n",
        "        match = re.search(r'F1 score.*?(\\d+\\.\\d+)', f1)\n",
        "        if match:\n",
        "            b = match.group(1)\n",
        "\n",
        "        print(result)\n",
        "        print(\"F1 Score =\", b)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "wvdFjnIkajIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}